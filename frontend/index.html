<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Chat</title>
    <link rel="stylesheet" href="/frontend/style.css">
</head>
<body>
    <div id="chat-container">
        <div id="chat-box"></div>
        <div id="status-area"></div>
        <div id="input-area">
            <input type="text" id="message-input" placeholder="ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›..." disabled>
            <button id="send-button" disabled>é€ä¿¡</button>
            <button id="mic-button" title="å¸¸æ™‚éŸ³å£°å…¥åŠ›ãƒ¢ãƒ¼ãƒ‰ã®åˆ‡ã‚Šæ›¿ãˆ" disabled>ğŸ¤</button>
        </div>
    </div>

    <script>
        const chatBox = document.getElementById('chat-box');
        const messageInput = document.getElementById('message-input');
        const sendButton = document.getElementById('send-button');
        const micButton = document.getElementById('mic-button');
        const statusArea = document.getElementById('status-area');

        const audioContext = new (window.AudioContext || window.webkitAudioContext)();
        const ws = new WebSocket(`ws://${window.location.host}/ws`);

        let audioQueue = [], isPlaying = false, currentAiMessageElement = null, userName = "U";
        let mediaRecorder, audioChunks = [], stream, analyser, dataArray, silenceTimer, isSpeaking = false, animationFrameId;
        let isContinuousMode = false;
        let aiTurnFinished = true; // AIã®ã‚¿ãƒ¼ãƒ³ãŒå®Œäº†ã—ãŸã‹ã‚’è¿½è·¡ã™ã‚‹ãƒ•ãƒ©ã‚°

        const VAD_THRESHOLD = 0.02;
        const SILENCE_DURATION_MS = 1500;

        ws.onopen = () => { enable_input(); };
        ws.onclose = () => { update_status("main", "ã‚µãƒ¼ãƒãƒ¼ã¨ã®æ¥ç¶šãŒåˆ‡ã‚Œã¾ã—ãŸã€‚"); disable_input(true); }
        ws.onerror = (e) => { update_status("main", "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"); disable_input(true); }

        ws.onmessage = (event) => {
            const message = JSON.parse(event.data);
            console.log("[RECEIVE]", message);
            
            switch (message.type) {
                case 'history': append_message(message.data.name, message.data.content); break;
                case 'user_transcription':
                    remove_status('audio-processing');
                    if (message.data && message.data.trim()) {
                        append_message(userName, message.data, true);
                        update_status('speaker-decision', 'ï¼ æ¬¡ã®è©±è€…ã‚’æ±ºã‚ã¦ã„ã¾ã™...');
                    } else {
                        update_status('asr-failed', 'éŸ³å£°ã‚’èªè­˜ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚', 2000);
                        if (isContinuousMode) {
                            setTimeout(startRecording, 100);
                        } else {
                            enable_input();
                        }
                    }
                    break;
                case 'next_speaker':
                    aiTurnFinished = false; // AIã®ã‚¿ãƒ¼ãƒ³é–‹å§‹
                    remove_status('speaker-decision');
                    remove_status('asr-failed');
                    update_status('ai_thinking', `ï¼ ${message.data}ãŒç™ºè¨€ã‚’è€ƒãˆã¦ã„ã¾ã™...`);
                    if (message.data !== userName) currentAiMessageElement = append_message(message.data, '');
                    break;
                case 'chunk': update_ai_message(message.data.content); break;
                // --- ã“ã“ã‹ã‚‰ãŒä¿®æ­£ç®‡æ‰€ ---
                case 'audio':
                    audioQueue.push(message.data);
                    if (!isPlaying) {
                        play_from_queue();
                    }
                    break;
                case 'stream_end':
                    finish_ai_turn();
                    break;
                // --- ã“ã“ã¾ã§ãŒä¿®æ­£ç®‡æ‰€ ---
                case 'image': append_image(message.url); break;
                case 'status_update': update_status(message.data.id, message.data.text); break;
                case 'status_remove': remove_status(message.data.id); break;
            }
        };
        
        const update_status = (id, text, timeout = 0) => { let el = document.getElementById(id); if (!el) { el = document.createElement('div'); el.id = id; el.className = 'status-item'; statusArea.appendChild(el); } el.textContent = text; if (timeout > 0) { setTimeout(() => { if (el) el.remove(); }, timeout); } };
        const remove_status = (id) => { const el = document.getElementById(id); if (el) el.remove(); };
        const clear_all_status = () => { statusArea.innerHTML = ''; };

        const enable_input = (isManual = true) => { messageInput.disabled = false; sendButton.disabled = false; micButton.disabled = false; if (!isContinuousMode || isManual) { update_status("main-prompt", "ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã¾ãŸã¯ãƒã‚¤ã‚¯ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚"); } };
        const disable_input = (isError = false) => { messageInput.disabled = true; sendButton.disabled = true; if (!isError) micButton.disabled = false; };

        const append_message = (name, text, isFinal = false) => { if (isFinal) { remove_status('temp-user-message'); } const el = document.createElement('div'); el.classList.add('message', name === userName ? 'user-message' : 'ai-message'); el.innerHTML = `<strong class="name">${name}</strong><p>${text}</p>`; chatBox.appendChild(el); chatBox.scrollTop = chatBox.scrollHeight; return el; };
        const append_temp_message = (text) => { update_status('temp-user-message', text); };
        const update_ai_message = (chunk) => { if (currentAiMessageElement) { currentAiMessageElement.querySelector('p').textContent += chunk; chatBox.scrollTop = chatBox.scrollHeight; } };
        const append_image = (url) => { const el = document.createElement('div'); el.classList.add('message', 'image-message'); el.innerHTML = `<img src="${url}" alt="Generated Image" onload="() => { chatBox.scrollTop = chatBox.scrollHeight; }">`; chatBox.appendChild(el); chatBox.scrollTop = chatBox.scrollHeight; };
        
        // --- ã“ã“ã‹ã‚‰ãŒä¿®æ­£ç®‡æ‰€ ---
        const finish_ai_turn = () => {
            currentAiMessageElement = null;
            remove_status('ai_thinking');
            aiTurnFinished = true;
            // å†ç”Ÿä¸­ã§ãªã‘ã‚Œã°ã€ã“ã®æ™‚ç‚¹ã§ãƒã‚¤ã‚¯ã‚’ONã«ã™ã‚‹
            if (!isPlaying) {
                if (isContinuousMode) {
                    startRecording();
                } else {
                    enable_input();
                }
            }
        };
        
        const play_from_queue = () => {
            if (audioQueue.length === 0) {
                isPlaying = false;
                // AIã®ã‚¿ãƒ¼ãƒ³ã‚‚çµ‚ã‚ã£ã¦ã„ã‚‹ãªã‚‰ãƒã‚¤ã‚¯ã‚’ON
                if (aiTurnFinished) {
                    if (isContinuousMode) { startRecording(); } else { enable_input(); }
                }
                return;
            }

            // å†ç”ŸãŒå§‹ã¾ã‚‹ç¬é–“ã«ãƒã‚¤ã‚¯ã‚’ãƒŸãƒ¥ãƒ¼ãƒˆ
            if (!isPlaying && isContinuousMode && stream) {
                stream.getAudioTracks().forEach(track => track.enabled = false);
                console.log("ğŸ¤ Mic Muted by AI speech");
            }
            isPlaying = true;

            const { audio, samplerate, dtype } = audioQueue.shift();
            // (ãƒ‡ã‚³ãƒ¼ãƒ‰å‡¦ç†)
            const rawAudio = atob(audio); const buffer = new ArrayBuffer(rawAudio.length); const view = new Uint8Array(buffer); for (let i = 0; i < rawAudio.length; i++) view[i] = rawAudio.charCodeAt(i); let float32Array; if (dtype === 'int16') { const i16 = new Int16Array(buffer); float32Array = new Float32Array(i16.length); for (let i = 0; i < i16.length; i++) float32Array[i] = i16[i] / 32768.0; } else if (dtype === 'float32') { float32Array = new Float32Array(buffer); } else { isPlaying = false; play_from_queue(); return; } const audioBuffer = audioContext.createBuffer(1, float32Array.length, samplerate); audioBuffer.copyToChannel(float32Array, 0);

            const source = audioContext.createBufferSource();
            source.buffer = audioBuffer;
            source.connect(audioContext.destination);
            source.onended = () => {
                // å†ç”ŸãŒçµ‚ã‚ã£ãŸã‚‰ã€ã‚­ãƒ¥ãƒ¼ã®æ¬¡ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’å†ç”Ÿ
                isPlaying = false;
                play_from_queue();
            };
            source.start();
        };
        // --- ã“ã“ã¾ã§ãŒä¿®æ­£ç®‡æ‰€ ---
        
        const sendMessage = () => { const message = messageInput.value; if (message && !messageInput.disabled) { ws.send(JSON.stringify({text: message})); append_message(userName, message); messageInput.value = ''; disable_input(); clear_all_status(); update_status('speaker-decision', 'ï¼ æ¬¡ã®è©±è€…ã‚’æ±ºã‚ã¦ã„ã¾ã™...'); } };
        
        const startVAD = () => { const source = audioContext.createMediaStreamSource(stream); analyser = audioContext.createAnalyser(); analyser.fftSize = 512; analyser.smoothingTimeConstant = 0.5; dataArray = new Uint8Array(analyser.frequencyBinCount); source.connect(analyser); detectSpeech(); };
        const detectSpeech = () => { if (!mediaRecorder || mediaRecorder.state !== "recording") { cancelAnimationFrame(animationFrameId); return; }; animationFrameId = requestAnimationFrame(detectSpeech); analyser.getByteTimeDomainData(dataArray); let sum = 0.0; for (const x of dataArray) { sum += Math.abs(x / 128.0 - 1.0); } const avg = sum / dataArray.length; if (avg > VAD_THRESHOLD) { if (!isSpeaking) { isSpeaking = true; append_temp_message('...èã„ã¦ã„ã¾ã™...'); } clearTimeout(silenceTimer); silenceTimer = null; } else if (isSpeaking) { if (!silenceTimer) { silenceTimer = setTimeout(() => { stopRecording(); }, SILENCE_DURATION_MS); } } };

        const startRecording = async () => {
            if (mediaRecorder && mediaRecorder.state === "recording") return;
            // --- ã“ã“ã‹ã‚‰ãŒä¿®æ­£ç®‡æ‰€ ---
            // éŒ²éŸ³é–‹å§‹å‰ã«å¿…ãšãƒã‚¤ã‚¯ã‚’æœ‰åŠ¹åŒ–ã™ã‚‹
            if (stream) {
                stream.getAudioTracks().forEach(track => {
                    if (!track.enabled) {
                        track.enabled = true;
                        console.log("ğŸ¤ Mic Unmuted for recording");
                    }
                });
            }
            // --- ã“ã“ã¾ã§ãŒä¿®æ­£ç®‡æ‰€ ---
            try {
                if (audioContext.state === 'suspended') { await audioContext.resume(); }
                stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                isSpeaking = false; silenceTimer = null;
                mediaRecorder = new MediaRecorder(stream, { mimeType: 'audio/webm' });
                audioChunks = [];
                mediaRecorder.ondataavailable = event => { audioChunks.push(event.data); };
                mediaRecorder.onstop = () => {
                    const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
                    if (audioBlob.size > 1000) {
                        ws.send(audioBlob);
                        clear_all_status();
                        update_status('audio-processing', 'ğŸ”Š éŸ³å£°è§£æä¸­...');
                    } else {
                        remove_status('temp-user-message');
                        if (isContinuousMode) {
                            startRecording();
                        } else {
                            enable_input();
                        }
                    }
                };
                mediaRecorder.start(); startVAD();
                micButton.classList.add('recording');
                micButton.textContent = "â¹ï¸";
                disable_input(); update_status('main-prompt', 'ğŸ¤ è©±ã—ã¦ãã ã•ã„...');
            } catch (err) { console.error("ãƒã‚¤ã‚¯ã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼:", err); update_status('main-prompt', 'ãƒã‚¤ã‚¯ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ãŒæ‹’å¦ã•ã‚Œã¾ã—ãŸã€‚'); enable_input(); }
        };

        const stopRecording = () => {
            if (mediaRecorder && mediaRecorder.state === "recording") { mediaRecorder.stop(); }
            if (stream) { stream.getTracks().forEach(track => track.stop()); stream = null; }
            if (animationFrameId) { cancelAnimationFrame(animationFrameId); }
            micButton.classList.remove('recording');
            micButton.textContent = isContinuousMode ? "ğŸ™ï¸" : "ğŸ¤";
            remove_status('temp-user-message');
        };
        
        micButton.onclick = () => {
            isContinuousMode = !isContinuousMode;
            micButton.classList.toggle('continuous-mode', isContinuousMode);
            if (isContinuousMode) {
                if (!micButton.classList.contains('recording') && !isPlaying) {
                    startRecording();
                }
            } else {
                if (micButton.classList.contains('recording')) {
                    stopRecording();
                }
                enable_input();
            }
        };

        sendButton.onclick = sendMessage;
        messageInput.addEventListener('keypress', (e) => { if (e.key === 'Enter') sendMessage() });
    </script>
</body>
</html>