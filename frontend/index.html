<!DOCTYPE html>
<html lang="ja">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Chat</title>
    <link rel="stylesheet" href="/frontend/style.css">
    <link rel="icon" href="/frontend/icon.ico">
</head>

<body>
    <!-- ‚ñº Â§âÊõ¥ÁÆáÊâÄ: ÂÖ®‰Ωì„ÇíÂõ≤„ÇÄ„Ç≥„É≥„ÉÜ„Éä„ÇíËøΩÂä† ‚ñº -->
    <div id="main-container">
        <div id="chat-container">
            <div id="chat-box"></div>
            <div id="status-area"></div>
            <div id="input-area">
                <input type="text" id="message-input" placeholder="„É°„ÉÉ„Çª„Éº„Ç∏„ÇíÂÖ•Âäõ..." disabled>
                <button id="send-button" disabled>ÈÄÅ‰ø°</button>
                <button id="mic-button" title="Â∏∏ÊôÇÈü≥Â£∞ÂÖ•Âäõ„É¢„Éº„Éâ„ÅÆÂàá„ÇäÊõø„Åà" disabled>üé§</button>
            </div>
        </div>
        <!-- ‚ñº Â§âÊõ¥ÁÆáÊâÄ: ÁîªÂÉèË°®Á§∫Áî®„ÅÆ„Éë„Éç„É´„ÇíËøΩÂä† ‚ñº -->
        <div id="image-panel" data-state="empty">
            <img id="context-image" src="" alt="Chat Context Image" data-state="empty">
        </div>
    </div>

    <script>
        const chatBox = document.getElementById('chat-box');
        const messageInput = document.getElementById('message-input');
        const sendButton = document.getElementById('send-button');
        const micButton = document.getElementById('mic-button');
        const statusArea = document.getElementById('status-area');
        // ‚ñº Â§âÊõ¥ÁÆáÊâÄ: ÁîªÂÉè„Éë„Éç„É´„ÅÆË¶ÅÁ¥†„ÇíÂèñÂæó ‚ñº
        const imagePanel = document.getElementById('image-panel');
        const contextImage = document.getElementById('context-image');

        const audioContext = new (window.AudioContext || window.webkitAudioContext)();
        const ws = new WebSocket(`ws://${window.location.host}/ws`);

        let audioQueue = [], isPlaying = false, currentAiMessageElement = null, userName = "U";
        let mediaRecorder, audioChunks = [], stream, analyser, dataArray, silenceTimer, isSpeaking = false, animationFrameId;
        let isContinuousMode = false;
        let aiTurnFinished = true;
        let recognition;
        let userInputMode = 'audio';
        let manualStop = false;
        let tempUserMessageElement = null;

        // (onopen, onclose, onerror „ÅØÂ§âÊõ¥„Å™„Åó)
        ws.onopen = () => { enable_input(); };
        ws.onclose = () => { update_status("main", "„Çµ„Éº„Éê„Éº„Å®„ÅÆÊé•Á∂ö„ÅåÂàá„Çå„Åæ„Åó„Åü„ÄÇ"); disable_input(true); }
        ws.onerror = (e) => { update_status("main", "„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü„ÄÇ"); disable_input(true); }


        ws.onmessage = (event) => {
            const message = JSON.parse(event.data);
            console.log("[RECEIVE]", message);

            switch (message.type) {
                // (config, user_transcription, history, next_speaker, chunk, audio, stream_end, status_update, status_remove „ÅØÂ§âÊõ¥„Å™„Åó)
                case 'config':
                    userName = message.data.user_name;
                    document.title = `${userName} - AI Chat`;
                    if (message.data.user_input_mode === 'text' || message.data.user_input_mode === 'browser') {
                        userInputMode = 'browser';
                        initializeSpeechRecognition();
                    } else {
                        userInputMode = message.data.user_input_mode;
                    }
                    micButton.title = `Â∏∏ÊôÇÈü≥Â£∞ÂÖ•Âäõ (${userInputMode === 'browser' ? '„Éñ„É©„Ç¶„Ç∂Ë™çË≠ò' : '„Çµ„Éº„Éê„ÉºË™çË≠ò'})`;
                    console.log(`GUI Input mode set to: ${userInputMode}`);
                    break;
                case 'user_transcription':
                    remove_temp_user_message();
                    if (message.data && message.data.trim()) {
                        append_message(userName, message.data);
                        update_status('speaker-decision', '‚û°Ô∏è Ê¨°„ÅÆË©±ËÄÖ„ÇíÊ±∫„ÇÅ„Å¶„ÅÑ„Åæ„Åô...');
                    } else {
                        update_status('asr-failed', 'Èü≥Â£∞„ÇíË™çË≠ò„Åß„Åç„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇ', 2000);
                        if (isContinuousMode) { setTimeout(startRecording, 100); } else { enable_input(); }
                    }
                    break;
                case 'history': append_message(message.data.name, message.data.content); break;
                case 'next_speaker':
                    aiTurnFinished = false;
                    remove_status('speaker-decision');
                    remove_status('asr-failed');
                    update_status('ai_thinking', `üß† ${message.data}„ÅåÁô∫Ë®Ä„ÇíËÄÉ„Åà„Å¶„ÅÑ„Åæ„Åô...`);
                    if (message.data !== userName) currentAiMessageElement = append_message(message.data, '');
                    break;
                case 'chunk': update_ai_message(message.data.content); break;
                case 'audio':
                    audioQueue.push(message.data);
                    if (!isPlaying) { play_from_queue(); }
                    break;
                case 'stream_end': finish_ai_turn(); break;
                
                // ‚ñº Â§âÊõ¥ÁÆáÊâÄ: 'image'„É°„ÉÉ„Çª„Éº„Ç∏„ÅÆÂá¶ÁêÜ„ÇíÂ§âÊõ¥ ‚ñº
                case 'image':
                    imagePanel.dataset.state = 'loaded';
                    contextImage.src = message.url;
                    break;

                case 'status_update': update_status(message.data.id, message.data.text); break;
                case 'status_remove': remove_status(message.data.id); break;
            }
        };

        // (initializeSpeechRecognition, update_status, remove_status, enable_input, disable_input „ÅØÂ§âÊõ¥„Å™„Åó)
        const initializeSpeechRecognition = () => {
            if (!("webkitSpeechRecognition" in window)) {
                alert("„Åì„ÅÆ„Éñ„É©„Ç¶„Ç∂„ÅØÈü≥Â£∞Ë™çË≠ò„Çí„Çµ„Éù„Éº„Éà„Åó„Å¶„ÅÑ„Åæ„Åõ„Çì„ÄÇ");
                micButton.disabled = true;
                return;
            }
            recognition = new webkitSpeechRecognition();
            recognition.continuous = false;
            recognition.interimResults = true;
            recognition.lang = 'ja-JP';

            recognition.onresult = (event) => {
                let interimTranscript = '';
                let finalTranscript = '';

                for (let i = event.resultIndex; i < event.results.length; ++i) {
                    if (event.results[i].isFinal) {
                        finalTranscript += event.results[i][0].transcript;
                    } else {
                        interimTranscript += event.results[i][0].transcript;
                    }
                }
                
                update_temp_user_message(interimTranscript);

                if (finalTranscript) {
                    const trimmedTranscript = finalTranscript.trim();
                    console.log("Final transcript:", trimmedTranscript);
                    
                    manualStop = true;
                    recognition.abort();
                    
                    remove_status('main-prompt'); 
                    remove_temp_user_message();
                    append_message(userName, trimmedTranscript);
                    
                    ws.send(JSON.stringify({ text: trimmedTranscript }));
                    disable_input();
                    update_status('speaker-decision', '‚û°Ô∏è Ê¨°„ÅÆË©±ËÄÖ„ÇíÊ±∫„ÇÅ„Å¶„ÅÑ„Åæ„Åô...');
                }
            };

            recognition.onend = () => {
                micButton.classList.remove('recording');
                micButton.textContent = isContinuousMode ? "üéôÔ∏è" : "üé§";
                remove_temp_user_message(); 
                remove_status('main-prompt');

                if (manualStop) {
                    manualStop = false;
                    return;
                }

                if (isContinuousMode && aiTurnFinished && !isPlaying) {
                    setTimeout(() => startRecording(), 100);
                } else if (!isContinuousMode) {
                    enable_input();
                }
            };

            recognition.onerror = (event) => {
                console.error("Speech recognition error:", event.error);
                if (event.error !== 'aborted') {
                    update_status('asr-failed', `Èü≥Â£∞Ë™çË≠ò„Ç®„É©„Éº: ${event.error}`, 3000);
                }
            };
        };
        const update_status = (id, text, timeout = 0) => { let el = document.getElementById(id); if (!el) { el = document.createElement('div'); el.id = id; el.className = 'status-item'; statusArea.appendChild(el); } el.textContent = text; if (timeout > 0) { setTimeout(() => { if (el) el.remove(); }, timeout); } };
        const remove_status = (id) => { const el = document.getElementById(id); if (el) el.remove(); };
        const enable_input = (isManual = true) => { messageInput.disabled = false; sendButton.disabled = false; micButton.disabled = false; if (!isContinuousMode || isManual) { remove_status("main-prompt"); update_status("main-prompt", "„É°„ÉÉ„Çª„Éº„Ç∏„ÇíÂÖ•Âäõ„Åæ„Åü„ÅØ„Éû„Ç§„ÇØ„ÇíÊäº„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"); } };
        const disable_input = (isError = false) => { messageInput.disabled = true; sendButton.disabled = true; if (!isError) micButton.disabled = false; remove_status("main-prompt"); };


        const append_message = (name, text) => { const el = document.createElement('div'); el.classList.add('message', name === userName ? 'user-message' : 'ai-message'); el.innerHTML = `<strong class="name">${name}</strong><p>${text}</p>`; chatBox.appendChild(el); chatBox.scrollTop = chatBox.scrollHeight; return el; };
        const update_ai_message = (chunk) => { if (currentAiMessageElement) { currentAiMessageElement.querySelector('p').textContent += chunk; chatBox.scrollTop = chatBox.scrollHeight; } };

        // ‚ñº Â§âÊõ¥ÁÆáÊâÄ: append_imageÈñ¢Êï∞„ÅØ‰∏çË¶Å„Å´„Å™„Å£„Åü„Åü„ÇÅÂâäÈô§ ‚ñº

        // (create/update/remove_temp_user_message, finish_ai_turn, play_from_queue, sendMessage, startRecording, stopRecording, micButton.onclick, startVAD, detectSpeech „ÅØÂ§âÊõ¥„Å™„Åó)
        const create_temp_user_message = () => {
            if (tempUserMessageElement) return;
            tempUserMessageElement = document.createElement('div');
            tempUserMessageElement.classList.add('message', 'user-message', 'temp-message');
            tempUserMessageElement.innerHTML = `<strong class="name">${userName}</strong><p></p>`;
            chatBox.appendChild(tempUserMessageElement);
            chatBox.scrollTop = chatBox.scrollHeight;
        };
        const update_temp_user_message = (text) => {
            if (!tempUserMessageElement) create_temp_user_message();
            if (tempUserMessageElement) {
                tempUserMessageElement.querySelector('p').textContent = text;
                chatBox.scrollTop = chatBox.scrollHeight;
            }
        };
        const remove_temp_user_message = () => {
            if (tempUserMessageElement) {
                tempUserMessageElement.remove();
                tempUserMessageElement = null;
            }
        };
        const finish_ai_turn = () => {
            currentAiMessageElement = null;
            remove_status('ai_thinking');
            aiTurnFinished = true;
            if (!isPlaying) {
                if (isContinuousMode) { startRecording(); } else { enable_input(); }
            }
        };
        const play_from_queue = () => {
            if (audioQueue.length === 0) {
                isPlaying = false;
                if (aiTurnFinished) {
                    if (isContinuousMode) { startRecording(); } else { enable_input(); }
                }
                return;
            }
            if (!isPlaying && isContinuousMode && stream && userInputMode !== 'browser') {
                stream.getAudioTracks().forEach(track => track.enabled = false);
                console.log("üé§ Mic Muted by AI speech (Server ASR)");
            }
            isPlaying = true;
            const { audio, samplerate, dtype } = audioQueue.shift();
            const rawAudio = atob(audio); const buffer = new ArrayBuffer(rawAudio.length); const view = new Uint8Array(buffer); for (let i = 0; i < rawAudio.length; i++) view[i] = rawAudio.charCodeAt(i); let float32Array; if (dtype === 'int16') { const i16 = new Int16Array(buffer); float32Array = new Float32Array(i16.length); for (let i = 0; i < i16.length; i++) float32Array[i] = i16[i] / 32768.0; } else if (dtype === 'float32') { float32Array = new Float32Array(buffer); } else { isPlaying = false; play_from_queue(); return; } const audioBuffer = audioContext.createBuffer(1, float32Array.length, samplerate); audioBuffer.copyToChannel(float32Array, 0);
            const source = audioContext.createBufferSource();
            source.buffer = audioBuffer;
            source.connect(audioContext.destination);
            source.onended = () => { isPlaying = false; play_from_queue(); };
            source.start();
        };
        const sendMessage = () => { const message = messageInput.value; if (message && !messageInput.disabled) { ws.send(JSON.stringify({ text: message })); append_message(userName, message); messageInput.value = ''; disable_input(); update_status('speaker-decision', '‚û°Ô∏è Ê¨°„ÅÆË©±ËÄÖ„ÇíÊ±∫„ÇÅ„Å¶„ÅÑ„Åæ„Åô...'); } };
        const startRecording = async () => {
            if (micButton.classList.contains('recording')) return;
            disable_input();
            micButton.classList.add('recording');
            micButton.textContent = "‚èπÔ∏è";
            update_status('main-prompt', 'üé§ Ë©±„Åó„Å¶„Åè„Å†„Åï„ÅÑ...');

            if (userInputMode === 'browser') {
                if (!recognition) { console.error("Recognition not initialized."); return; }
                try {
                    if (audioContext.state === 'suspended') { await audioContext.resume(); }
                    create_temp_user_message();
                    recognition.start();
                } catch (e) {
                    console.warn("Recognition might be already active:", e);
                }
            } else {
                if (stream) {
                    stream.getAudioTracks().forEach(track => {
                        if (!track.enabled) { track.enabled = true; console.log("üé§ Mic Unmuted for recording (Server ASR)"); }
                    });
                }
                try {
                    if (audioContext.state === 'suspended') { await audioContext.resume(); }
                    stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                    isSpeaking = false; silenceTimer = null;
                    mediaRecorder = new MediaRecorder(stream, { mimeType: 'audio/webm' });
                    audioChunks = [];
                    mediaRecorder.ondataavailable = event => { audioChunks.push(event.data); };
                    mediaRecorder.onstop = () => {
                        const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
                        if (audioBlob.size > 1000) { ws.send(audioBlob); } else { remove_temp_user_message(); if (isContinuousMode) { startRecording(); } else { enable_input(); } }
                    };
                    mediaRecorder.start();
                    startVAD();
                } catch (err) {
                    console.error("„Éû„Ç§„ÇØ„Ç¢„ÇØ„Çª„Çπ„Ç®„É©„Éº:", err);
                    update_status('main-prompt', '„Éû„Ç§„ÇØ„Å∏„ÅÆ„Ç¢„ÇØ„Çª„Çπ„ÅåÊãíÂê¶„Åï„Çå„Åæ„Åó„Åü„ÄÇ');
                    enable_input();
                    micButton.classList.remove('recording');
                    micButton.textContent = "üé§";
                }
            }
        };
        const stopRecording = () => {
            micButton.classList.remove('recording');
            micButton.textContent = isContinuousMode ? "üéôÔ∏è" : "üé§";
            remove_temp_user_message();
            remove_status('main-prompt');

            if (userInputMode === 'browser') {
                if (recognition) { manualStop = true; recognition.abort(); }
            } else {
                if (mediaRecorder && mediaRecorder.state === "recording") { mediaRecorder.stop(); }
                if (stream) { stream.getTracks().forEach(track => track.stop()); stream = null; }
                if (animationFrameId) { cancelAnimationFrame(animationFrameId); }
            }
        };
        micButton.onclick = () => {
            isContinuousMode = !isContinuousMode;
            micButton.classList.toggle('continuous-mode', isContinuousMode);
            if (isContinuousMode) {
                if (!micButton.classList.contains('recording') && !isPlaying) { startRecording(); }
            } else {
                if (micButton.classList.contains('recording')) { stopRecording(); }
                enable_input();
            }
        };
        const startVAD = () => { const source = audioContext.createMediaStreamSource(stream); analyser = audioContext.createAnalyser(); analyser.fftSize = 512; analyser.smoothingTimeConstant = 0.5; dataArray = new Uint8Array(analyser.frequencyBinCount); source.connect(analyser); detectSpeech(); };
        const detectSpeech = () => { if (!mediaRecorder || mediaRecorder.state !== "recording") { cancelAnimationFrame(animationFrameId); return; }; animationFrameId = requestAnimationFrame(detectSpeech); analyser.getByteTimeDomainData(dataArray); let sum = 0.0; for (const x of dataArray) { sum += Math.abs(x / 128.0 - 1.0); } const avg = sum / dataArray.length; if (avg > VAD_THRESHOLD) { if (!isSpeaking) { isSpeaking = true; update_status('main-prompt', '...ËÅû„ÅÑ„Å¶„ÅÑ„Åæ„Åô...'); } clearTimeout(silenceTimer); silenceTimer = null; } else if (isSpeaking) { if (!silenceTimer) { silenceTimer = setTimeout(() => { stopRecording(); }, SILENCE_DURATION_MS); } } };
        sendButton.onclick = sendMessage;
        messageInput.addEventListener('keypress', (e) => { if (e.key === 'Enter') sendMessage() });
    </script>
</body>
</html>